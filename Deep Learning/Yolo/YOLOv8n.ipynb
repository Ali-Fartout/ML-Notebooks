{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecbb0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3c84683",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor = torch.randint(1, 256, (1,3, 640, 640), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f87954ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 640, 640])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3348a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size, padding, stride = 1):\n",
    "        super(Conv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = nn.BatchNorm2d(x.shape[1], affine=False)(x)\n",
    "        x = nn.SiLU()(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b69b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,shortcut,in_channels):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1_bottleneck = Conv2d(in_channels=in_channels, out_channels=in_channels // 2, kernel_size=3, padding=1, stride=1).to(dtype=torch.float32)\n",
    "        self.conv2_bottleneck = Conv2d(in_channels=in_channels // 2, out_channels=in_channels, kernel_size=3, padding=1, stride=1).to(dtype=torch.float32)\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.clone()\n",
    "        x = self.conv1_bottleneck(x)\n",
    "        x = self.conv2_bottleneck(x)\n",
    "        \n",
    "        if self.shortcut: \n",
    "            x = x + y\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daea003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2f(nn.Module):\n",
    "    def __init__(self,n,shortcut,in_channels):\n",
    "        super(C2f, self).__init__()\n",
    "        self.n = n\n",
    "        self.shortcut = shortcut\n",
    "        self.conv1 = Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=1, padding=0, stride=1).to(dtype=torch.float32)\n",
    "        self.bottleneck_blocks = nn.ModuleList([Bottleneck(self.shortcut, in_channels // 2) for _ in range(self.n)])\n",
    "        self.conv2 = Conv2d(in_channels=(((self.n+2)*in_channels)//2), out_channels= in_channels , kernel_size=1, padding=0, stride=1).to(dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x1, x2 = torch.chunk(x, 2, dim=1)\n",
    "        x2 = torch.cat((x2, x1), dim=1)\n",
    "\n",
    "        for bottleneck_block in self.bottleneck_blocks:\n",
    "             x1 = bottleneck_block(x1)\n",
    "             x2 = torch.cat((x2, x1), dim=1)\n",
    "        \n",
    "        x = x2\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b3dbd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPF(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SPPF, self).__init__()\n",
    "        self.conv1 =  Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=1, padding=0, stride=1)\n",
    "        self.conv2 =  Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=1, padding=0, stride=1)\n",
    "        self.pool_sizes = [5, 9, 13]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        pooled_outputs = []\n",
    "#         x1 = torch.cat((x, y), dim=1)\n",
    "        for pool_size in self.pool_sizes:\n",
    "            pooled = nn.functional.max_pool2d(x, kernel_size=pool_size)\n",
    "            pooled_outputs.append(pooled)\n",
    "        else:\n",
    "            pooled_outputs.append(x)\n",
    "        \n",
    "        \n",
    "        x = torch.cat(pooled_outputs, dim=1)\n",
    "        x = self.conv2(x)\n",
    "        print(\"1=\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d49e4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Backbone, self).__init__()\n",
    "\n",
    "        self.p1 = Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1, stride=2).to(dtype=torch.float32)\n",
    "        self.p2 = Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=2).to(dtype=torch.float32)\n",
    "        \n",
    "        self.c2f_1 = C2f(1, True,  32)\n",
    "        \n",
    "        self.p3 =  Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=2).to(dtype=torch.float32)\n",
    "        self.c2f_2 = C2f(2, True,  64)\n",
    "        \n",
    "        self.p4 =  Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=2).to(dtype=torch.float32)\n",
    "        self.c2f_3 = C2f(2, True,  128)\n",
    "        self.p5 =  Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=2).to(dtype=torch.float32)\n",
    "        self.c2f_4 = C2f(2, True,  256)\n",
    "\n",
    "        self.sppf = SPPF(256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.p1(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.c2f_1(x) \n",
    "        x = self.p3(x)\n",
    "        y = self.c2f_2(x)\n",
    "        x = self.p4(y)\n",
    "        z = self.c2f_3(x)\n",
    "        x = self.p5(z)\n",
    "        x = self.c2f_4(x)\n",
    "        x = self.sppf(x)\n",
    "        \n",
    "        return y, z, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60a3041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor = torch.randint(1, 256, (1,3, 640, 640), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a08d3e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 4 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26084\\4231343585.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26084\\990695584.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc2f_4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26084\\3828335095.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpooled_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1=\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "b = Backbone()\n",
    "a,c,e = b(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65855f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
