{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel \n",
    "from pathlib import Path\n",
    "roberta_model = \"roberta-large\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset, load_from_disk ,concatenate_datasets, DatasetDict , Sequence , Value , Features , ClassLabel\n",
    "conll = load_dataset(\"conll2003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each label to it's number\n",
    "tags = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "tags_names = ClassLabel(num_classes=len(tags) , names=tags)\n",
    "def create_tag_names(batch):\n",
    "    return {'ner_tags_names' : [tags_names.int2str(idx) for idx in batch['ner_tags']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll = conll.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6d9c2f2afb4b4abfebe94da808c3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ee591d2c0c42f8b3530c6289bcc48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53809bda2b224524abb85b148694efbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove additional columns\n",
    "conll['train'] = conll['train'].remove_columns(['pos_tags','chunk_tags','id'])\n",
    "conll['test'] = conll['test'].remove_columns(['pos_tags','chunk_tags','id'])\n",
    "conll['validation'] = conll['validation'].remove_columns(['pos_tags','chunk_tags','id'])\n",
    "conll.save_to_disk(\"Conell_en.hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'ner_tags_names'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'ner_tags_names'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'ner_tags_names'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training without Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoTokenizer , RobertaConfig , AutoConfig , TrainingArguments , DataCollatorForTokenClassification , Trainer\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class Roberta(RobertaPreTrainedModel):\n",
    "    \n",
    "    class_config = RobertaConfig\n",
    "    \n",
    "    def __init__(self , config):\n",
    "        \n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.roberta = RobertaModel(config , add_pooling_layer=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        self.classifier = nn.Linear(config.hidden_size , config.num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self , input_ids=None , attention_mask=None , token_type_ids=None , labels=None , **kwargs):\n",
    "\n",
    "        outputs = self.roberta(input_ids , attention_mask=attention_mask , token_type_ids=token_type_ids , **kwargs)\n",
    "\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1 , self.num_labels) , labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss = loss,\n",
    "            logits = logits,\n",
    "            hidden_states = outputs.hidden_states,\n",
    "            attentions = outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model, add_prefix_space=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = roberta_tokenizer(examples[\"tokens\"], truncation=True,\n",
    "    is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags_names\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                #  if a word convert more than one toekn, ignore except first token\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_token = label[word_idx]\n",
    "                # Use the label map to get the numerical value for each entity\n",
    "                label_ids.append(tag2index[label_token])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = conll['train']['ner_tags_names']\n",
    "ner_tag_names = set(tag for tags in ner_tags for tag in tags)\n",
    "                \n",
    "index2tag = {idx: tag for idx, tag in enumerate(ner_tag_names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(ner_tag_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels,batched=True,\n",
    "                      remove_columns=['tokens' , 'ner_tags' , 'ner_tags_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e357e538ade04a5791feead429c65a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76945c61b9994bb1b8ad40985971a6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f2b8487b26414788b8d3e8384ff5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_data = encode(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = preds.shape\n",
    "        labels_list, preds_list = [], []\n",
    "        for batch_idx in range(batch_size):\n",
    "            example_labels, example_preds = [], []\n",
    "            for seq_idx in range(seq_len):\n",
    "                # Ignore label IDs = -100\n",
    "                if label_ids[batch_idx, seq_idx] != -100:\n",
    "                    example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                    example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "            labels_list.append(example_labels)\n",
    "            preds_list.append(example_preds)\n",
    "        return preds_list, labels_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "# init training args\n",
    "num_epochs = 4\n",
    "batch_size = 24\n",
    "logging_steps = len(encoded_data['train']) // batch_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=roberta_model, log_level=\"error\", num_train_epochs=num_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    eval_accumulation_steps=10,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    seed=42,\n",
    "    logging_strategy=\"steps\", evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
    "    logging_steps=logging_steps, push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for model\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    roberta_config  = AutoConfig.from_pretrained(\n",
    "    \"roberta-large\",\n",
    "    num_labels = len(index2tag),\n",
    "    id2label = index2tag,\n",
    "    label2id = tag2index\n",
    "    )\n",
    "    return Roberta.from_pretrained(roberta_model, config=roberta_config,cache_dir=Path.cwd()).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score,recall_score,precision_score,accuracy_score\n",
    "import wandb\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "    eval_pred.label_ids)\n",
    "    wandb.log({\"f1\": f1_score(y_true, y_pred),\"Recall\":recall_score(y_true, y_pred),\"Precision\":precision_score(y_true, y_pred),\"Accuracy\":accuracy_score(y_true, y_pred)})\n",
    "    return {\"f1\": f1_score(y_true, y_pred),\"Recall\":recall_score(y_true, y_pred),\"Precision\":precision_score(y_true, y_pred),\"Accuracy\":accuracy_score(y_true,y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                    data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                    train_dataset=encoded_data['train'],\n",
    "                    eval_dataset=encoded_data['validation'],\n",
    "                    tokenizer=roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mali-fartout\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c41c4f29fb4a4e88e134495d8bfde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333333327028, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\ML\\ML Notebooks\\Deep Learning\\Lora\\wandb\\run-20231226_155236-tuleusaq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ali-fartout/Roberta/runs/tuleusaq' target=\"_blank\">without-lora</a></strong> to <a href='https://wandb.ai/ali-fartout/Roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ali-fartout/Roberta' target=\"_blank\">https://wandb.ai/ali-fartout/Roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ali-fartout/Roberta/runs/tuleusaq' target=\"_blank\">https://wandb.ai/ali-fartout/Roberta/runs/tuleusaq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a576fcebb045483ba0ce5c58d498c271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0955, 'learning_rate': 3.7521331058020484e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ea0a5766b34935b947056d2182b20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04328341409564018, 'eval_f1': 0.9372141372141373, 'eval_Recall': 0.9483338943116796, 'eval_Precision': 0.9263521288837745, 'eval_Accuracy': 0.9903819944706203, 'eval_runtime': 9.0545, 'eval_samples_per_second': 358.937, 'eval_steps_per_second': 15.02, 'epoch': 1.0}\n",
      "{'loss': 0.0338, 'learning_rate': 2.5042662116040955e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50ff0e160564d11a0e774d1a25d109f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03254313766956329, 'eval_f1': 0.9524687735769972, 'eval_Recall': 0.9560753954897341, 'eval_Precision': 0.9488892600634709, 'eval_Accuracy': 0.9918811572758071, 'eval_runtime': 9.0805, 'eval_samples_per_second': 357.908, 'eval_steps_per_second': 14.977, 'epoch': 2.0}\n",
      "{'loss': 0.0173, 'learning_rate': 1.2563993174061433e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67565dce4e914a8c82d1188d31d97d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.031557872891426086, 'eval_f1': 0.9570243779844183, 'eval_Recall': 0.9612924941097274, 'eval_Precision': 0.9527939949958298, 'eval_Accuracy': 0.9922900198590398, 'eval_runtime': 9.097, 'eval_samples_per_second': 357.259, 'eval_steps_per_second': 14.95, 'epoch': 3.0}\n",
      "{'loss': 0.0085, 'learning_rate': 8.532423208191127e-08, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5754b39ef5487690ffcfb30ad63c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.029325664043426514, 'eval_f1': 0.9638574423480084, 'eval_Recall': 0.9671827667452036, 'eval_Precision': 0.9605549055657697, 'eval_Accuracy': 0.9935750165491998, 'eval_runtime': 9.1179, 'eval_samples_per_second': 356.441, 'eval_steps_per_second': 14.916, 'epoch': 4.0}\n",
      "{'train_runtime': 719.6786, 'train_samples_per_second': 78.04, 'train_steps_per_second': 3.257, 'train_loss': 0.03870046309015869, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d3770f3b554fc9bd0e62a220bfd23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▄▅█</td></tr><tr><td>Precision</td><td>▁▆▆█</td></tr><tr><td>Recall</td><td>▁▄▆█</td></tr><tr><td>eval/Accuracy</td><td>▁▄▅█</td></tr><tr><td>eval/Precision</td><td>▁▆▆█</td></tr><tr><td>eval/Recall</td><td>▁▄▆█</td></tr><tr><td>eval/f1</td><td>▁▅▆█</td></tr><tr><td>eval/loss</td><td>█▃▂▁</td></tr><tr><td>eval/runtime</td><td>▁▄▆█</td></tr><tr><td>eval/samples_per_second</td><td>█▅▃▁</td></tr><tr><td>eval/steps_per_second</td><td>█▅▃▁</td></tr><tr><td>f1</td><td>▁▅▆█</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▁▃▃▃▆▆▆████</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.99358</td></tr><tr><td>Precision</td><td>0.96055</td></tr><tr><td>Recall</td><td>0.96718</td></tr><tr><td>eval/Accuracy</td><td>0.99358</td></tr><tr><td>eval/Precision</td><td>0.96055</td></tr><tr><td>eval/Recall</td><td>0.96718</td></tr><tr><td>eval/f1</td><td>0.96386</td></tr><tr><td>eval/loss</td><td>0.02933</td></tr><tr><td>eval/runtime</td><td>9.1179</td></tr><tr><td>eval/samples_per_second</td><td>356.441</td></tr><tr><td>eval/steps_per_second</td><td>14.916</td></tr><tr><td>f1</td><td>0.96386</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>2344</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0085</td></tr><tr><td>train/total_flos</td><td>5177078465615838.0</td></tr><tr><td>train/train_loss</td><td>0.0387</td></tr><tr><td>train/train_runtime</td><td>719.6786</td></tr><tr><td>train/train_samples_per_second</td><td>78.04</td></tr><tr><td>train/train_steps_per_second</td><td>3.257</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">without-lora</strong> at: <a href='https://wandb.ai/ali-fartout/Roberta/runs/tuleusaq' target=\"_blank\">https://wandb.ai/ali-fartout/Roberta/runs/tuleusaq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231226_155236-tuleusaq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# monitoring\n",
    "wandb.init(project=\"Roberta\",name='without-lora')\n",
    "result = trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354319369"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.num_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoConfig, AutoModel \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def model_init():\n",
    "    roberta_config  = AutoConfig.from_pretrained(\n",
    "    \"roberta-large\",\n",
    "    num_labels = len(index2tag),\n",
    "    id2label = index2tag,\n",
    "    label2id = tag2index\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n",
    "    )\n",
    "    original_model = Roberta.from_pretrained(roberta_model, config=roberta_config).to(device)\n",
    "    return  get_peft_model(original_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,853,449 || all params: 355,901,458 || trainable%: 0.5207758940959438\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_lora = model_init()\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "num_epochs = 4\n",
    "batch_size = 32\n",
    "logging_steps = len(encoded_data['train']) // batch_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lorass\", log_level=\"error\", num_train_epochs=num_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=1e-3,\n",
    "    seed=42,\n",
    "    logging_strategy=\"steps\", evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
    "    logging_steps=logging_steps, push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=encoded_data['train'],\n",
    "    eval_dataset=encoded_data['validation'],\n",
    "    tokenizer=roberta_tokenizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ee27614bad447fbc8a8a1611b79bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333333327028, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\ML\\ML Notebooks\\Deep Learning\\Lora\\wandb\\run-20231226_175206-g0n3cxf1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ali-fartout/Roberta/runs/g0n3cxf1' target=\"_blank\">lora</a></strong> to <a href='https://wandb.ai/ali-fartout/Roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ali-fartout/Roberta' target=\"_blank\">https://wandb.ai/ali-fartout/Roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ali-fartout/Roberta/runs/g0n3cxf1' target=\"_blank\">https://wandb.ai/ali-fartout/Roberta/runs/g0n3cxf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655f55bca6134b8a8f9115d031f5828d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1108, 'learning_rate': 0.0007505694760820045, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9316d1ee70143de8fecaeb3f021ef55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04405393451452255, 'eval_f1': 0.9235382308845577, 'eval_Recall': 0.9330191854594413, 'eval_Precision': 0.9142480211081794, 'eval_Accuracy': 0.987773061796659, 'eval_runtime': 9.1971, 'eval_samples_per_second': 353.37, 'eval_steps_per_second': 11.09, 'epoch': 1.0}\n",
      "{'loss': 0.0431, 'learning_rate': 0.0005011389521640091, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f7cc366d8f40a3ab40147708a0681b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.032071053981781006, 'eval_f1': 0.952969676718737, 'eval_Recall': 0.9599461460787614, 'eval_Precision': 0.9460938795820202, 'eval_Accuracy': 0.9923289591526809, 'eval_runtime': 9.1989, 'eval_samples_per_second': 353.304, 'eval_steps_per_second': 11.088, 'epoch': 2.0}\n",
      "{'loss': 0.0302, 'learning_rate': 0.0002517084282460137, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebac50c32b7f461ba7f8bf724c47f9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.029818153008818626, 'eval_f1': 0.9573007367716009, 'eval_Recall': 0.9621339616290812, 'eval_Precision': 0.9525158280573143, 'eval_Accuracy': 0.9925431252677076, 'eval_runtime': 9.1969, 'eval_samples_per_second': 353.38, 'eval_steps_per_second': 11.091, 'epoch': 3.0}\n",
      "{'loss': 0.0217, 'learning_rate': 2.2779043280182233e-06, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a93a6222d24177b420e0bc6c0bdf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.026289232075214386, 'eval_f1': 0.9632235905168803, 'eval_Recall': 0.9675193537529452, 'eval_Precision': 0.9589658048373645, 'eval_Accuracy': 0.993613955842841, 'eval_runtime': 9.315, 'eval_samples_per_second': 348.901, 'eval_steps_per_second': 10.95, 'epoch': 4.0}\n",
      "{'train_runtime': 555.7154, 'train_samples_per_second': 101.066, 'train_steps_per_second': 3.16, 'train_loss': 0.051398200680902714, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Roberta\",name='lora')\n",
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
